{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6bf7834-dd0f-4df4-9742-fed244a712b0",
   "metadata": {},
   "source": [
    "## Medical NLP Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc11ec63-557e-4029-9b47-58d5d118ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "df = load_dataset(\"har1/MTS_Dialogue-Clinical_Note\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1ac98f-ea05-4c46-b7d1-f027d61ec2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"har1/HealthScribe-Clinical_Note_Generator\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"har1/HealthScribe-Clinical_Note_Generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f07d41-8b45-45cd-a32e-8015d5cfb13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symptoms: occasional backaches\n",
      "Diagnosis: whiplash injury\n",
      "History of Patient: Involved in a motor vehicle accident on September 1st, 2001, while driving a Ford Taurus from Cheadle Hulme to Manchester, hit head on steering wheel, experienced pain in neck and back immediately after accident, sought medical care at Moss Bank Accident and Emergency, received advice and sent home\n",
      "Plan of Action: N/A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ Prepare the conversation as one string\n",
    "input_text = (\n",
    "    \"Physician: How are you feeling today?\\n\"\n",
    "    \"Patient: I’m doing better, but I still have some discomfort now and then.\\n\"\n",
    "    \"Physician: I understand you were in a car accident last September. Can you walk me through what happened?\\n\"\n",
    "    \"Patient: Yes, it was on September 1st, around 12:30 in the afternoon. I was driving from Cheadle Hulme to Manchester when I had to stop in traffic. Out of nowhere, another car hit me from behind, which pushed my car into the one in front.\\n\"\n",
    "    \"Physician: That sounds like a strong impact. Were you wearing your seatbelt?\\n\"\n",
    "    \"Patient: Yes, I always do.\\n\"\n",
    "    \"Physician: What did you feel immediately after the accident?\\n\"\n",
    "    \"Patient: At first, I was just shocked. But then I realized I had hit my head on the steering wheel, and I could feel pain in my neck and back almost right away.\\n\"\n",
    "    \"Physician: Did you seek medical attention at that time?\\n\"\n",
    "    \"Patient: Yes, I went to Moss Bank Accident and Emergency. They checked me over and said it was a whiplash injury, but they didn’t do any X-rays. They just gave me some advice and sent me home.\\n\"\n",
    "    \"Physician: How did things progress after that?\\n\"\n",
    "    \"Patient: The first four weeks were rough. My neck and back pain were really bad—I had trouble sleeping and had to take painkillers regularly. It started improving after that, but I had to go through ten sessions of physiotherapy to help with the stiffness and discomfort.\\n\"\n",
    "    \"Physician: That makes sense. Are you still experiencing pain now?\\n\"\n",
    "    \"Patient: It’s not constant, but I do get occasional backaches. It’s nothing like before, though.\\n\"\n",
    "    \"Physician: That’s good to hear. Have you noticed any other effects, like anxiety while driving or difficulty concentrating?\\n\"\n",
    "    \"Patient: No, nothing like that. I don’t feel nervous driving, and I haven’t had any emotional issues from the accident.\\n\"\n",
    "    \"Physician: And how has this impacted your daily life? Work, hobbies, anything like that?\\n\"\n",
    "    \"Patient: I had to take a week off work, but after that, I was back to my usual routine. It hasn’t really stopped me from doing anything.\\n\"\n",
    "    \"Physician: That’s encouraging. Let’s go ahead and do a physical examination to check your mobility and any lingering pain.\\n\"\n",
    "    \"[Physical Examination Conducted]\\n\"\n",
    "    \"Physician: Everything looks good. Your neck and back have a full range of movement, and there’s no tenderness or signs of lasting damage. Your muscles and spine seem to be in good condition.\\n\"\n",
    "    \"Patient: That’s a relief!\\n\"\n",
    "    \"Physician: Yes, your recovery so far has been quite positive. Given your progress, I’d expect you to make a full recovery within six months of the accident. There are no signs of long-term damage or degeneration.\\n\"\n",
    "    \"Patient: That’s great to hear. So, I don’t need to worry about this affecting me in the future?\\n\"\n",
    "    \"Physician: That’s right. I don’t foresee any long-term impact on your work or daily life. If anything changes or you experience worsening symptoms, you can always come back for a follow-up. But at this point, you’re on track for a full recovery.\\n\"\n",
    "    \"Patient: Thank you, doctor. I appreciate it.\\n\"\n",
    "    \"Physician: You’re very welcome, Ms. Jones. Take care, and don’t hesitate to reach out if you need anything.\"\n",
    ")\n",
    "\n",
    "\n",
    "# 2️⃣ Tokenize\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,      # just in case it's long\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# 3️⃣ Generate output\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,   # adjust as needed\n",
    "    num_beams=4,          # beam search for better quality\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# 4️⃣ Decode\n",
    "generated_note = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c73d3f-743c-4e90-8764-25f591ccdf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model output (not valid JSON):\n",
      "Symptoms: occasional backaches\n",
      "Diagnosis: whiplash injury\n",
      "History of Patient: Involved in a motor vehicle accident on September 1st, 2001, while driving a Ford Taurus from Cheadle Hulme to Manchester, hit head on steering wheel, experienced pain in neck and back immediately after accident, sought medical care at Moss Bank Accident and Emergency, received advice and sent home\n",
      "Plan of Action: N/A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 1️⃣ Prepare the conversation as one string\n",
    "input_text = (\n",
    "    \"Physician: How are you feeling today?\\n\"\n",
    "    \"Patient: I’m doing better, but I still have some discomfort now and then.\\n\"\n",
    "    \"Physician: I understand you were in a car accident last September. Can you walk me through what happened?\\n\"\n",
    "    \"Patient: Yes, it was on September 1st, around 12:30 in the afternoon. I was driving from Cheadle Hulme to Manchester when I had to stop in traffic. Out of nowhere, another car hit me from behind, which pushed my car into the one in front.\\n\"\n",
    "    \"Physician: That sounds like a strong impact. Were you wearing your seatbelt?\\n\"\n",
    "    \"Patient: Yes, I always do.\\n\"\n",
    "    \"Physician: What did you feel immediately after the accident?\\n\"\n",
    "    \"Patient: At first, I was just shocked. But then I realized I had hit my head on the steering wheel, and I could feel pain in my neck and back almost right away.\\n\"\n",
    "    \"Physician: Did you seek medical attention at that time?\\n\"\n",
    "    \"Patient: Yes, I went to Moss Bank Accident and Emergency. They checked me over and said it was a whiplash injury, but they didn’t do any X-rays. They just gave me some advice and sent me home.\\n\"\n",
    "    \"Physician: How did things progress after that?\\n\"\n",
    "    \"Patient: The first four weeks were rough. My neck and back pain were really bad—I had trouble sleeping and had to take painkillers regularly. It started improving after that, but I had to go through ten sessions of physiotherapy to help with the stiffness and discomfort.\\n\"\n",
    "    \"Physician: That makes sense. Are you still experiencing pain now?\\n\"\n",
    "    \"Patient: It’s not constant, but I do get occasional backaches. It’s nothing like before, though.\\n\"\n",
    "    \"Physician: That’s good to hear. Have you noticed any other effects, like anxiety while driving or difficulty concentrating?\\n\"\n",
    "    \"Patient: No, nothing like that. I don’t feel nervous driving, and I haven’t had any emotional issues from the accident.\\n\"\n",
    "    \"Physician: And how has this impacted your daily life? Work, hobbies, anything like that?\\n\"\n",
    "    \"Patient: I had to take a week off work, but after that, I was back to my usual routine. It hasn’t really stopped me from doing anything.\\n\"\n",
    "    \"Physician: That’s encouraging. Let’s go ahead and do a physical examination to check your mobility and any lingering pain.\\n\"\n",
    "    \"[Physical Examination Conducted]\\n\"\n",
    "    \"Physician: Everything looks good. Your neck and back have a full range of movement, and there’s no tenderness or signs of lasting damage. Your muscles and spine seem to be in good condition.\\n\"\n",
    "    \"Patient: That’s a relief!\\n\"\n",
    "    \"Physician: Yes, your recovery so far has been quite positive. Given your progress, I’d expect you to make a full recovery within six months of the accident. There are no signs of long-term damage or degeneration.\\n\"\n",
    "    \"Patient: That’s great to hear. So, I don’t need to worry about this affecting me in the future?\\n\"\n",
    "    \"Physician: That’s right. I don’t foresee any long-term impact on your work or daily life. If anything changes or you experience worsening symptoms, you can always come back for a follow-up. But at this point, you’re on track for a full recovery.\\n\"\n",
    "    \"Patient: Thank you, doctor. I appreciate it.\\n\"\n",
    "    \"Physician: You’re very welcome, Ms. Jones. Take care, and don’t hesitate to reach out if you need anything.\"\n",
    ")\n",
    "\n",
    "# 2️⃣ Add instruction for JSON output\n",
    "prompt = (\n",
    "    input_text +\n",
    "    \"\\n\\nPlease extract the structured medical summary in the following JSON format only:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"Patient_Name\": \"Janet Jones\",\\n'\n",
    "    '  \"Symptoms\": [\"Neck pain\", \"Back pain\", \"Head impact\"],\\n'\n",
    "    '  \"Diagnosis\": \"Whiplash injury\",\\n'\n",
    "    '  \"Treatment\": [\"10 physiotherapy sessions\", \"Painkillers\"],\\n'\n",
    "    '  \"Current_Status\": \"Occasional backache\",\\n'\n",
    "    '  \"Prognosis\": \"Full recovery expected within six months\"\\n'\n",
    "    \"}\"\n",
    ")\n",
    "\n",
    "# 3️⃣ Tokenize\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# 4️⃣ Generate JSON output\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=300,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# 5️⃣ Decode and parse JSON\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Try parsing to dict\n",
    "try:\n",
    "    generated_json = json.loads(generated_text)\n",
    "    print(json.dumps(generated_json, indent=2))\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Raw model output (not valid JSON):\")\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ecc9de1-91b8-4408-8d64-f26c483000ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anxiety' 'Normal' 'Depression' 'Suicidal' 'Stress' 'Bipolar'\n",
      " 'Personality disorder']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# If you haven’t loaded it yet\n",
    "df = pd.read_csv(\"Combined Data.csv\")   # or pd.read_excel(\"your_file.xlsx\")\n",
    "\n",
    "# Unique entries in the 'status' column\n",
    "print(df['status'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06372cd3-3b35-42f2-83f3-a2daa94daeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anxiety' 'Normal']\n",
      "oh my gosh\n",
      "trouble sleeping, confused mind, restless heart. All out of tune\n",
      "All wrong, back off dear, forward doubt. Stay in a restless and restless place\n",
      "I've shifted my focus to something else but I'm still worried\n",
      "I'm restless and restless, it's been a month now, boy. What do you mean?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# drop rows where status is in the unwanted list\n",
    "df = df[~df['status'].isin(['Suicidal', 'Bipolar', 'Personality disorder'])]\n",
    "\n",
    "# replace Depression and Stress with Anxiety\n",
    "df['status'] = df['status'].replace({\n",
    "    'Depression': 'Anxiety',\n",
    "    'Stress': 'Anxiety'\n",
    "})\n",
    "\n",
    "# (optional) check unique values again\n",
    "print(df['status'].unique())\n",
    "for i in range(5):\n",
    "    print(df[\"statement\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92a26f6b-b97e-4c0f-8cd4-9e633d2b5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV (without the pandas index column)\n",
    "balanced_df.to_csv(\"cleaned_dataset2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0f53b79-8ed4-4668-aa6a-035ee7d1c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cleaned_dataset2.csv\")\n",
    "df = df[['statement','status']]  # keep only the two needed columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c4d467e-8075-4848-b11f-1eafac981b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anxiety' 'Normal']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "balanced_df['label'] = le.fit_transform(balanced_df['status'])\n",
    "print(le.classes_)  # e.g. ['Anxiety' 'Normal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d007c256-dbf4-4b3a-b2b5-6efca6874b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "Anxiety    21832\n",
       "Normal     16343\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2823be3d-3db8-4703-993d-af92ac9173ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status\n",
      "Anxiety    8000\n",
      "Normal     8000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your df has 'status' column with classes and original data\n",
    "# Downsample each class to 8000 rows randomly\n",
    "\n",
    "anxiety_df = df[df[\"status\"] == \"Anxiety\"].sample(n=8000, random_state=42)\n",
    "normal_df = df[df[\"status\"] == \"Normal\"].sample(n=8000, random_state=42)\n",
    "\n",
    "# Concatenate balanced data into a new DataFrame\n",
    "balanced_df = pd.concat([anxiety_df, normal_df]).reset_index(drop=True)\n",
    "\n",
    "print(balanced_df[\"status\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8924e338-ac26-46f6-99e6-dbb73cf08180",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (2.20.1)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Collecting keras>=3.10.0 (from tensorflow<2.21,>=2.20->tf-keras)\n",
      "  Using cached keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.2.6)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "Using cached keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-3.11.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.11.3 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.5.3 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.20.0 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.17.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82aceccf-504f-495b-8cd8-c27ca84f94ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (4.56.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from transformers) (0.35.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\menda\\miniconda3\\envs\\mednlp\\lib\\site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a91cf0-9d1e-4212-8cb1-081683446a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "def extract_medical_info_generic(conversation_text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generic extraction function that works with your existing model.\n",
    "    Requires your model to be instruction-tuned or fine-tuned for extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a detailed prompt for extraction\n",
    "    extraction_prompt = f\"\"\"<|system|>\n",
    "You are a medical information extraction assistant. Extract information from conversations and return ONLY valid JSON.\n",
    "\n",
    "<|user|>\n",
    "Extract the following from this medical conversation:\n",
    "1. Patient_Name (string)\n",
    "2. Symptoms (list of strings)\n",
    "3. Diagnosis (string)\n",
    "4. Treatment (list of strings)\n",
    "5. Current_Status (string)\n",
    "6. Prognosis (string)\n",
    "\n",
    "Conversation:\n",
    "{conversation_text}\n",
    "\n",
    "Return ONLY a JSON object with these exact keys.\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        extraction_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024  # Increased for longer conversations\n",
    "    )\n",
    "\n",
    "    # Generate with better parameters for structured output\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        num_beams=5,\n",
    "        temperature=0.3,  # Lower temperature for more consistent output\n",
    "        do_sample=False,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract JSON from response\n",
    "    try:\n",
    "        import re\n",
    "        json_match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', generated_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            medical_info = json.loads(json_match.group())\n",
    "            return medical_info\n",
    "        else:\n",
    "            # Fallback: try to parse the whole response\n",
    "            return json.loads(generated_text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Could not parse JSON. Raw output:\")\n",
    "        print(generated_text)\n",
    "        return parse_text_to_json(generated_text)\n",
    "\n",
    "def parse_text_to_json(text):\n",
    "    \"\"\"\n",
    "    Fallback parser when model doesn't return proper JSON.\n",
    "    Attempts to extract structured information from free text.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    medical_info = {\n",
    "        \"Patient_Name\": None,\n",
    "        \"Symptoms\": [],\n",
    "        \"Diagnosis\": None,\n",
    "        \"Treatment\": [],\n",
    "        \"Current_Status\": None,\n",
    "        \"Prognosis\": None\n",
    "    }\n",
    "    \n",
    "    # Try to find each field in the text\n",
    "    lines = text.split('\\n')\n",
    "    current_field = None\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if 'patient_name' in line.lower() or 'patient name' in line.lower():\n",
    "            match = re.search(r'[\"\\']([^\"\\']+)[\"\\']|:\\s*(.+?)(?:,|\\n|$)', line, re.IGNORECASE)\n",
    "            if match:\n",
    "                medical_info['Patient_Name'] = match.group(1) or match.group(2)\n",
    "        \n",
    "        elif 'symptoms' in line.lower():\n",
    "            # Extract list items\n",
    "            items = re.findall(r'[\"\\']([^\"\\']+)[\"\\']', line)\n",
    "            if items:\n",
    "                medical_info['Symptoms'] = items\n",
    "        \n",
    "        elif 'diagnosis' in line.lower():\n",
    "            match = re.search(r'[\"\\']([^\"\\']+)[\"\\']|:\\s*(.+?)(?:,|\\n|$)', line, re.IGNORECASE)\n",
    "            if match:\n",
    "                medical_info['Diagnosis'] = match.group(1) or match.group(2)\n",
    "        \n",
    "        elif 'treatment' in line.lower():\n",
    "            items = re.findall(r'[\"\\']([^\"\\']+)[\"\\']', line)\n",
    "            if items:\n",
    "                medical_info['Treatment'] = items\n",
    "        \n",
    "        elif 'current_status' in line.lower() or 'current status' in line.lower():\n",
    "            match = re.search(r'[\"\\']([^\"\\']+)[\"\\']|:\\s*(.+?)(?:,|\\n|$)', line, re.IGNORECASE)\n",
    "            if match:\n",
    "                medical_info['Current_Status'] = match.group(1) or match.group(2)\n",
    "        \n",
    "        elif 'prognosis' in line.lower():\n",
    "            match = re.search(r'[\"\\']([^\"\\']+)[\"\\']|:\\s*(.+?)(?:,|\\n|$)', line, re.IGNORECASE)\n",
    "            if match:\n",
    "                medical_info['Prognosis'] = match.group(1) or match.group(2)\n",
    "    \n",
    "    return medical_info\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN USAGE EXAMPLE\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample conversation\n",
    "    input_text = \"\"\"\n",
    "    Physician: How are you feeling today?\n",
    "    Patient: I'm doing better, but I still have some discomfort now and then.\n",
    "    Physician: I understand you were in a car accident last September. Can you walk me through what happened?\n",
    "    Patient: Yes, it was on September 1st, around 12:30 in the afternoon. I was driving from Cheadle Hulme to Manchester when I had to stop in traffic. Out of nowhere, another car hit me from behind, which pushed my car into the one in front.\n",
    "    Physician: That sounds like a strong impact. Were you wearing your seatbelt?\n",
    "    Patient: Yes, I always do.\n",
    "    Physician: What did you feel immediately after the accident?\n",
    "    Patient: At first, I was just shocked. But then I realized I had hit my head on the steering wheel, and I could feel pain in my neck and back almost right away.\n",
    "    Physician: Did you seek medical attention at that time?\n",
    "    Patient: Yes, I went to Moss Bank Accident and Emergency. They checked me over and said it was a whiplash injury, but they didn't do any X-rays. They just gave me some advice and sent me home.\n",
    "    Physician: How did things progress after that?\n",
    "    Patient: The first four weeks were rough. My neck and back pain were really bad—I had trouble sleeping and had to take painkillers regularly. It started improving after that, but I had to go through ten sessions of physiotherapy to help with the stiffness and discomfort.\n",
    "    Physician: That makes sense. Are you still experiencing pain now?\n",
    "    Patient: It's not constant, but I do get occasional backaches. It's nothing like before, though.\n",
    "    Physician: That's good to hear. Have you noticed any other effects, like anxiety while driving or difficulty concentrating?\n",
    "    Patient: No, nothing like that. I don't feel nervous driving, and I haven't had any emotional issues from the accident.\n",
    "    Physician: And how has this impacted your daily life? Work, hobbies, anything like that?\n",
    "    Patient: I had to take a week off work, but after that, I was back to my usual routine. It hasn't really stopped me from doing anything.\n",
    "    Physician: That's encouraging. Let's go ahead and do a physical examination to check your mobility and any lingering pain.\n",
    "    [Physical Examination Conducted]\n",
    "    Physician: Everything looks good. Your neck and back have a full range of movement, and there's no tenderness or signs of lasting damage. Your muscles and spine seem to be in good condition.\n",
    "    Patient: That's a relief!\n",
    "    Physician: Yes, your recovery so far has been quite positive. Given your progress, I'd expect you to make a full recovery within six months of the accident. There are no signs of long-term damage or degeneration.\n",
    "    Patient: That's great to hear. So, I don't need to worry about this affecting me in the future?\n",
    "    Physician: That's right. I don't foresee any long-term impact on your work or daily life. If anything changes or you experience worsening symptoms, you can always come back for a follow-up. But at this point, you're on track for a full recovery.\n",
    "    Patient: Thank you, doctor. I appreciate it.\n",
    "    Physician: You're very welcome, Ms. Jones. Take care, and don't hesitate to reach out if you need anything.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPTION 1: Using FLAN-T5 (Recommended)\")\n",
    "    print(\"=\" * 80)\n",
    "    result = extract_medical_info_with_llm(input_text)\n",
    "    print(json.dumps(result, indent=2))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OPTION 2: Using Your Existing Model\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Uncomment and use with your tokenizer and model:\")\n",
    "    print(\"# result = extract_medical_info_generic(input_text, tokenizer, model)\")\n",
    "    print(\"# print(json.dumps(result, indent=2))\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OPTION 3: Using OpenAI API\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Uncomment and provide API key:\")\n",
    "    print(\"# result = extract_with_openai(input_text, api_key='your-api-key')\")\n",
    "    print(\"# print(json.dumps(result, indent=2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9571ca6-6bc8-49b2-aaca-4c53cd40b5ce",
   "metadata": {},
   "source": [
    "## Sentiment & Intent Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11506014-eec4-4eef-a369-de13dfea6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    TFAutoModel,\n",
    "    DataCollatorWithPadding,\n",
    "    create_optimizer\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"cleaned_dataset2.csv\")\n",
    "\n",
    "# If you only have 'status' column, you'll need to create intent labels\n",
    "# For now, I'll show you how to handle both sentiment and intent\n",
    "\n",
    "# Option A: If you have both columns in CSV\n",
    "if 'intent' in df.columns:\n",
    "    df = df[[\"statement\", \"status\", \"intent\"]].dropna()\n",
    "else:\n",
    "    # Option B: Create intent labels based on patterns (you'll need to customize this)\n",
    "    def assign_intent(row):\n",
    "        statement = str(row['statement']).lower()\n",
    "        status = row['status']\n",
    "        \n",
    "        # Rule-based intent assignment (customize based on your data)\n",
    "        if status == 'anxiety':\n",
    "            if any(word in statement for word in ['help', 'what should', 'worried', 'scared']):\n",
    "                return 'seeking_reassurance'\n",
    "            elif any(word in statement for word in ['can\\'t', 'unable', 'difficult']):\n",
    "                return 'expressing_difficulty'\n",
    "            else:\n",
    "                return 'expressing_concern'\n",
    "        else:  # neutral\n",
    "            if any(word in statement for word in ['good', 'fine', 'okay', 'well']):\n",
    "                return 'expressing_confidence'\n",
    "            elif any(word in statement for word in ['think', 'believe', 'feel']):\n",
    "                return 'sharing_opinion'\n",
    "            else:\n",
    "                return 'making_statement'\n",
    "    \n",
    "    df['intent'] = df.apply(assign_intent, axis=1)\n",
    "    df = df[[\"statement\", \"status\", \"intent\"]].dropna()\n",
    "\n",
    "# Encode both labels\n",
    "sentiment_le = LabelEncoder()\n",
    "intent_le = LabelEncoder()\n",
    "\n",
    "df[\"sentiment_labels\"] = sentiment_le.fit_transform(df[\"status\"])\n",
    "df[\"intent_labels\"] = intent_le.fit_transform(df[\"intent\"])\n",
    "\n",
    "print(f\"Sentiment classes: {sentiment_le.classes_}\")\n",
    "print(f\"Intent classes: {intent_le.classes_}\")\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Tokenizer\n",
    "# ============================================================================\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_data(batch):\n",
    "    return tokenizer(batch[\"statement\"], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "tokenized_train = train_ds.map(preprocess_data, batched=True)\n",
    "tokenized_test = test_ds.map(preprocess_data, batched=True)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Data Collator\n",
    "# ============================================================================\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Convert to tf.data.Dataset with Multiple Labels\n",
    "# ============================================================================\n",
    "\n",
    "tf_train_dataset = tokenized_train.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"sentiment_labels\", \"intent_labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_test.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"sentiment_labels\", \"intent_labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Build Multi-Task Model\n",
    "# ============================================================================\n",
    "\n",
    "class MultiTaskModel(keras.Model):\n",
    "    def __init__(self, num_sentiments, num_intents, model_name=\"distilbert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained DistilBERT\n",
    "        from transformers import TFAutoModel\n",
    "        self.distilbert = TFAutoModel.from_pretrained(model_name, from_pt=True)\n",
    "        \n",
    "        # Sentiment classification head\n",
    "        self.sentiment_dropout = keras.layers.Dropout(0.1)\n",
    "        self.sentiment_classifier = keras.layers.Dense(\n",
    "            num_sentiments, \n",
    "            activation=None, \n",
    "            name='sentiment_output'\n",
    "        )\n",
    "        \n",
    "        # Intent classification head\n",
    "        self.intent_dropout = keras.layers.Dropout(0.1)\n",
    "        self.intent_classifier = keras.layers.Dense(\n",
    "            num_intents, \n",
    "            activation=None, \n",
    "            name='intent_output'\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        # Get DistilBERT outputs\n",
    "        outputs = self.distilbert(inputs, training=training)\n",
    "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        cls_output = sequence_output[:, 0, :]  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Sentiment branch\n",
    "        sentiment_output = self.sentiment_dropout(cls_output, training=training)\n",
    "        sentiment_logits = self.sentiment_classifier(sentiment_output)\n",
    "        \n",
    "        # Intent branch\n",
    "        intent_output = self.intent_dropout(cls_output, training=training)\n",
    "        intent_logits = self.intent_classifier(intent_output)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_output': sentiment_logits,\n",
    "            'intent_output': intent_logits\n",
    "        }\n",
    "\n",
    "# Instantiate model\n",
    "num_sentiments = len(sentiment_le.classes_)\n",
    "num_intents = len(intent_le.classes_)\n",
    "\n",
    "model = MultiTaskModel(num_sentiments, num_intents)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Compile with Multiple Losses\n",
    "# ============================================================================\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "batches_per_epoch = len(tf_train_dataset)\n",
    "total_train_steps = batches_per_epoch * num_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5, \n",
    "    num_warmup_steps=0, \n",
    "    num_train_steps=total_train_steps\n",
    ")\n",
    "\n",
    "# Define losses and metrics for both tasks\n",
    "losses = {\n",
    "    'sentiment_output': keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    'intent_output': keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'sentiment_output': [keras.metrics.SparseCategoricalAccuracy(name='sentiment_accuracy')],\n",
    "    'intent_output': [keras.metrics.SparseCategoricalAccuracy(name='intent_accuracy')]\n",
    "}\n",
    "\n",
    "# You can weight the losses if one task is more important\n",
    "loss_weights = {\n",
    "    'sentiment_output': 1.0,\n",
    "    'intent_output': 1.0\n",
    "}\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=losses,\n",
    "    metrics=metrics,\n",
    "    loss_weights=loss_weights\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Train Model\n",
    "# ============================================================================\n",
    "\n",
    "# Build the model by calling it once\n",
    "dummy_input = {\n",
    "    'input_ids': tf.constant([[0] * 128]),\n",
    "    'attention_mask': tf.constant([[1] * 128])\n",
    "}\n",
    "_ = model(dummy_input)\n",
    "\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs=num_epochs\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: Save Model and Encoders\n",
    "# ============================================================================\n",
    "\n",
    "model.save_weights(\"./multi_task_model/model_weights\")\n",
    "tokenizer.save_pretrained(\"./multi_task_model\")\n",
    "joblib.dump(sentiment_le, \"./multi_task_model/sentiment_encoder.pkl\")\n",
    "joblib.dump(intent_le, \"./multi_task_model/intent_encoder.pkl\")\n",
    "\n",
    "# Save model config\n",
    "config = {\n",
    "    'num_sentiments': num_sentiments,\n",
    "    'num_intents': num_intents,\n",
    "    'model_name': model_name\n",
    "}\n",
    "with open(\"./multi_task_model/config.json\", 'w') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "print(\"\\n✅ Model training complete!\")\n",
    "print(f\"Sentiment classes: {list(sentiment_le.classes_)}\")\n",
    "print(f\"Intent classes: {list(intent_le.classes_)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: Inference Function to Get JSON Output\n",
    "# ============================================================================\n",
    "\n",
    "def predict_with_json(text, model, tokenizer, sentiment_le, intent_le):\n",
    "    \"\"\"\n",
    "    Predict sentiment and intent, return as JSON\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    outputs = model(inputs, training=False)\n",
    "    \n",
    "    # Get predictions\n",
    "    sentiment_logits = outputs['sentiment_output']\n",
    "    intent_logits = outputs['intent_output']\n",
    "    \n",
    "    sentiment_pred = tf.argmax(sentiment_logits, axis=1).numpy()[0]\n",
    "    intent_pred = tf.argmax(intent_logits, axis=1).numpy()[0]\n",
    "    \n",
    "    # Decode labels\n",
    "    sentiment_label = sentiment_le.inverse_transform([sentiment_pred])[0]\n",
    "    intent_label = intent_le.inverse_transform([intent_pred])[0]\n",
    "    \n",
    "    # Format intent label nicely\n",
    "    intent_formatted = intent_label.replace('_', ' ').title()\n",
    "    sentiment_formatted = sentiment_label.capitalize()\n",
    "    \n",
    "    # Return JSON\n",
    "    result = {\n",
    "        \"Sentiment\": sentiment_formatted,\n",
    "        \"Intent\": intent_formatted\n",
    "    }\n",
    "    \n",
    "    return json.dumps(result, indent=2)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: Test the Model\n",
    "# ============================================================================\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"I'm really worried about the upcoming exam\",\n",
    "    \"Everything is going well today\",\n",
    "    \"I can't handle this stress anymore\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING MODEL WITH JSON OUTPUT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\nInput: {text}\")\n",
    "    result = predict_with_json(text, model, tokenizer, sentiment_le, intent_le)\n",
    "    print(f\"Output: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2ace9-05ee-451e-a597-2300b8addcf6",
   "metadata": {},
   "source": [
    "## SOAP NOTES GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58879663-890d-4ca6-83c7-29d6bdd8ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SOAP Note Generation System\n",
    "Implements multiple approaches for converting medical transcripts to SOAP notes\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ============================================================================\n",
    "# APPROACH 1: Rule-Based + NER (Named Entity Recognition)\n",
    "# ============================================================================\n",
    "\n",
    "class RuleBasedSOAPGenerator:\n",
    "    \"\"\"\n",
    "    Rule-based system using pattern matching and medical keywords\n",
    "    Best for: Quick prototyping, interpretable results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define keyword patterns for each SOAP section\n",
    "        self.subjective_keywords = {\n",
    "            'complaint': ['hurt', 'pain', 'ache', 'discomfort', 'feeling', 'symptom'],\n",
    "            'history': ['accident', 'injury', 'started', 'began', 'weeks ago', 'months ago', \n",
    "                       'experienced', 'had', 'was', 'happened']\n",
    "        }\n",
    "        \n",
    "        self.objective_keywords = {\n",
    "            'physical_exam': ['examination', 'range of motion', 'tenderness', 'swelling', \n",
    "                            'temperature', 'blood pressure', 'heart rate', 'respiratory rate'],\n",
    "            'observations': ['appears', 'looks', 'gait', 'posture', 'condition', 'normal', 'abnormal']\n",
    "        }\n",
    "        \n",
    "        self.assessment_keywords = {\n",
    "            'diagnosis': ['diagnosis', 'diagnosed with', 'condition', 'injury', 'disease', \n",
    "                         'whiplash', 'strain', 'sprain', 'fracture'],\n",
    "            'severity': ['mild', 'moderate', 'severe', 'improving', 'worsening', 'stable']\n",
    "        }\n",
    "        \n",
    "        self.plan_keywords = {\n",
    "            'treatment': ['treatment', 'medication', 'therapy', 'physiotherapy', 'surgery', \n",
    "                         'continue', 'prescribe', 'recommend', 'analgesic', 'painkiller'],\n",
    "            'follow_up': ['follow-up', 'return', 'come back', 'appointment', 'check-up', \n",
    "                         'monitor', 'if', 'worsens', 'persists']\n",
    "        }\n",
    "    \n",
    "    def extract_sentences_by_keywords(self, text: str, keywords: List[str]) -> List[str]:\n",
    "        \"\"\"Extract sentences containing specific keywords\"\"\"\n",
    "        sentences = re.split(r'[.!?]', text)\n",
    "        matching_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if any(keyword.lower() in sentence.lower() for keyword in keywords):\n",
    "                matching_sentences.append(sentence)\n",
    "        \n",
    "        return matching_sentences\n",
    "    \n",
    "    def extract_subjective(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract Subjective section\"\"\"\n",
    "        complaint_sentences = self.extract_sentences_by_keywords(\n",
    "            text, self.subjective_keywords['complaint']\n",
    "        )\n",
    "        history_sentences = self.extract_sentences_by_keywords(\n",
    "            text, self.subjective_keywords['history']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"Chief_Complaint\": ' '.join(complaint_sentences[:2]) if complaint_sentences else \"Not documented\",\n",
    "            \"History_of_Present_Illness\": ' '.join(history_sentences[:3]) if history_sentences else \"Not documented\"\n",
    "        }\n",
    "    \n",
    "    def extract_objective(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract Objective section\"\"\"\n",
    "        exam_sentences = self.extract_sentences_by_keywords(\n",
    "            text, self.objective_keywords['physical_exam']\n",
    "        )\n",
    "        observation_sentences = self.extract_sentences_by_keywords(\n",
    "            text, self.objective_keywords['observations']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"Physical_Exam\": ' '.join(exam_sentences) if exam_sentences else \"Not documented\",\n",
    "            \"Observations\": ' '.join(observation_sentences) if observation_sentences else \"Not documented\"\n",
    "        }\n",
    "    \n",
    "    def extract_assessment(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract Assessment section\"\"\"\n",
    "        diagnosis_sentences = self.extract_sentences_by_keywords(\n",
    "            text, self.assessment_keywords['diagnosis']\n",
    "        )\n",
    "        severity_sentences = self.extract_sentences_by_keywords(\n",
    "            text, self.assessment_keywords['severity']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"Diagnosis\": ' '.join(diagnosis_sentences) if diagnosis_sentences else \"Not documented\",\n",
    "            \"Severity\": ' '.join(severity_sentences) if severity_sentences else \"Not documented\"\n",
    "        }\n",
    "    \n",
    "    def extract_plan(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract Plan section\"\"\"\n",
    "        treatment_sentences = self.extract_sentences_by_keywords(\n",
    "            text, self.plan_keywords['treatment']\n",
    "        )\n",
    "        followup_sentences = self.extract_sentences_by_keywords(\n",
    "            text, self.plan_keywords['follow_up']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"Treatment\": ' '.join(treatment_sentences) if treatment_sentences else \"Not documented\",\n",
    "            \"Follow_Up\": ' '.join(followup_sentences) if followup_sentences else \"Not documented\"\n",
    "        }\n",
    "    \n",
    "    def generate_soap_note(self, transcript: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate complete SOAP note from transcript\"\"\"\n",
    "        return {\n",
    "            \"Subjective\": self.extract_subjective(transcript),\n",
    "            \"Objective\": self.extract_objective(transcript),\n",
    "            \"Assessment\": self.extract_assessment(transcript),\n",
    "            \"Plan\": self.extract_plan(transcript)\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# APPROACH 2: Transformer-Based Fine-Tuning (T5/BART)\n",
    "# ============================================================================\n",
    "\n",
    "class TransformerSOAPGenerator:\n",
    "    \"\"\"\n",
    "    Uses pre-trained transformer models fine-tuned on SOAP note generation\n",
    "    Best for: High accuracy, handles complex medical language\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"t5-base\"):\n",
    "        from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = TFT5ForConditionalGeneration.from_pretrained(model_name, from_pt=True)\n",
    "    \n",
    "    def prepare_training_data(self, csv_path: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Prepare training data from CSV with columns:\n",
    "        - transcript: full conversation text\n",
    "        - subjective: JSON string\n",
    "        - objective: JSON string\n",
    "        - assessment: JSON string\n",
    "        - plan: JSON string\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Create training examples for each SOAP section\n",
    "        training_data = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            transcript = row['transcript']\n",
    "            \n",
    "            # Create training pairs for each section\n",
    "            sections = {\n",
    "                'Subjective': row['subjective'],\n",
    "                'Objective': row['objective'],\n",
    "                'Assessment': row['assessment'],\n",
    "                'Plan': row['plan']\n",
    "            }\n",
    "            \n",
    "            for section_name, section_content in sections.items():\n",
    "                input_text = f\"extract {section_name.lower()} from medical transcript: {transcript}\"\n",
    "                output_text = section_content\n",
    "                training_data.append({\n",
    "                    'input': input_text,\n",
    "                    'output': output_text\n",
    "                })\n",
    "        \n",
    "        train_data, val_data = train_test_split(training_data, test_size=0.2, random_state=42)\n",
    "        return train_data, val_data\n",
    "    \n",
    "    def train(self, train_data: List[Dict], val_data: List[Dict], epochs: int = 3):\n",
    "        \"\"\"\n",
    "        Fine-tune T5 model on SOAP note generation\n",
    "        \"\"\"\n",
    "        import tensorflow as tf\n",
    "        from transformers import create_optimizer\n",
    "        \n",
    "        # Tokenize training data\n",
    "        def tokenize_data(examples):\n",
    "            model_inputs = self.tokenizer(\n",
    "                [ex['input'] for ex in examples],\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "            \n",
    "            labels = self.tokenizer(\n",
    "                [ex['output'] for ex in examples],\n",
    "                max_length=256,\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "            \n",
    "            model_inputs['labels'] = labels['input_ids']\n",
    "            return model_inputs\n",
    "        \n",
    "        # Prepare datasets\n",
    "        train_encodings = tokenize_data(train_data)\n",
    "        val_encodings = tokenize_data(val_data)\n",
    "        \n",
    "        # Create TF datasets\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            {\n",
    "                'input_ids': train_encodings['input_ids'],\n",
    "                'attention_mask': train_encodings['attention_mask']\n",
    "            },\n",
    "            train_encodings['labels']\n",
    "        )).batch(8)\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            {\n",
    "                'input_ids': val_encodings['input_ids'],\n",
    "                'attention_mask': val_encodings['attention_mask']\n",
    "            },\n",
    "            val_encodings['labels']\n",
    "        )).batch(8)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        num_train_steps = len(train_data) // 8 * epochs\n",
    "        optimizer, schedule = create_optimizer(\n",
    "            init_lr=5e-5,\n",
    "            num_warmup_steps=0,\n",
    "            num_train_steps=num_train_steps\n",
    "        )\n",
    "        \n",
    "        # Compile and train\n",
    "        self.model.compile(optimizer=optimizer)\n",
    "        self.model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs\n",
    "        )\n",
    "    \n",
    "    def generate_section(self, transcript: str, section: str) -> str:\n",
    "        \"\"\"Generate a specific SOAP section\"\"\"\n",
    "        input_text = f\"extract {section.lower()} from medical transcript: {transcript}\"\n",
    "        \n",
    "        input_ids = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"tf\",\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=256,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def generate_soap_note(self, transcript: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate complete SOAP note\"\"\"\n",
    "        sections = ['Subjective', 'Objective', 'Assessment', 'Plan']\n",
    "        soap_note = {}\n",
    "        \n",
    "        for section in sections:\n",
    "            section_text = self.generate_section(transcript, section)\n",
    "            # Parse JSON if the model outputs JSON format\n",
    "            try:\n",
    "                soap_note[section] = json.loads(section_text)\n",
    "            except json.JSONDecodeError:\n",
    "                # If not JSON, structure it manually\n",
    "                soap_note[section] = {\"content\": section_text}\n",
    "        \n",
    "        return soap_note\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# APPROACH 3: Sequence Labeling with BERT (Token Classification)\n",
    "# ============================================================================\n",
    "\n",
    "class BERTSOAPLabeler:\n",
    "    \"\"\"\n",
    "    Uses BERT for sequence labeling to tag each sentence with SOAP category\n",
    "    Best for: Precise sentence-level categorization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        from transformers import TFBertForTokenClassification, BertTokenizerFast\n",
    "        \n",
    "        # Label mapping for SOAP sections + subsections\n",
    "        self.label_map = {\n",
    "            'O': 0,  # Outside any SOAP section\n",
    "            'S-COMPLAINT': 1,  # Subjective - Chief Complaint\n",
    "            'S-HISTORY': 2,    # Subjective - History\n",
    "            'O-EXAM': 3,       # Objective - Physical Exam\n",
    "            'O-OBS': 4,        # Objective - Observations\n",
    "            'A-DIAG': 5,       # Assessment - Diagnosis\n",
    "            'A-SEV': 6,        # Assessment - Severity\n",
    "            'P-TREAT': 7,      # Plan - Treatment\n",
    "            'P-FOLLOW': 8      # Plan - Follow-up\n",
    "        }\n",
    "        \n",
    "        self.id_to_label = {v: k for k, v in self.label_map.items()}\n",
    "        \n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.model = TFBertForTokenClassification.from_pretrained(\n",
    "            'bert-base-uncased',\n",
    "            num_labels=len(self.label_map),\n",
    "            from_pt=True\n",
    "        )\n",
    "    \n",
    "    def prepare_training_data(self, annotated_transcripts: List[Dict]) -> tuple:\n",
    "        \"\"\"\n",
    "        Prepare data where each sentence is labeled with SOAP category\n",
    "        Format: [{'text': 'sentence', 'label': 'S-COMPLAINT'}, ...]\n",
    "        \"\"\"\n",
    "        tokenized_inputs = []\n",
    "        labels = []\n",
    "        \n",
    "        for transcript in annotated_transcripts:\n",
    "            sentences = transcript['sentences']\n",
    "            sentence_labels = transcript['labels']\n",
    "            \n",
    "            # Tokenize sentences\n",
    "            encoding = self.tokenizer(\n",
    "                sentences,\n",
    "                is_split_into_words=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "            \n",
    "            # Convert labels to IDs\n",
    "            label_ids = [self.label_map[label] for label in sentence_labels]\n",
    "            \n",
    "            # Align labels with tokens\n",
    "            word_ids = encoding.word_ids()\n",
    "            aligned_labels = []\n",
    "            \n",
    "            for word_id in word_ids:\n",
    "                if word_id is None:\n",
    "                    aligned_labels.append(-100)  # Ignore padding\n",
    "                else:\n",
    "                    aligned_labels.append(label_ids[word_id])\n",
    "            \n",
    "            tokenized_inputs.append(encoding)\n",
    "            labels.append(aligned_labels)\n",
    "        \n",
    "        return tokenized_inputs, labels\n",
    "    \n",
    "    def train(self, train_data, val_data, epochs=3):\n",
    "        \"\"\"Train the BERT token classifier\"\"\"\n",
    "        import tensorflow as tf\n",
    "        from transformers import create_optimizer\n",
    "        \n",
    "        # Prepare TF datasets\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(train_data[0]),\n",
    "            train_data[1]\n",
    "        )).batch(8)\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(val_data[0]),\n",
    "            val_data[1]\n",
    "        )).batch(8)\n",
    "        \n",
    "        # Setup training\n",
    "        num_train_steps = len(train_data[0]) // 8 * epochs\n",
    "        optimizer, _ = create_optimizer(\n",
    "            init_lr=5e-5,\n",
    "            num_warmup_steps=0,\n",
    "            num_train_steps=num_train_steps\n",
    "        )\n",
    "        \n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "        \n",
    "        self.model.fit(train_dataset, validation_data=val_dataset, epochs=epochs)\n",
    "    \n",
    "    def predict_soap_labels(self, transcript: str) -> List[tuple]:\n",
    "        \"\"\"Predict SOAP labels for each sentence\"\"\"\n",
    "        sentences = re.split(r'[.!?]', transcript)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            sentences,\n",
    "            is_split_into_words=True,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        \n",
    "        # Predict\n",
    "        outputs = self.model(inputs)\n",
    "        predictions = tf.argmax(outputs.logits, axis=-1)\n",
    "        \n",
    "        # Map predictions to labels\n",
    "        labeled_sentences = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            label_id = predictions[i, 1].numpy()  # Skip [CLS] token\n",
    "            label = self.id_to_label[label_id]\n",
    "            labeled_sentences.append((sentence, label))\n",
    "        \n",
    "        return labeled_sentences\n",
    "    \n",
    "    def generate_soap_note(self, transcript: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate SOAP note from labeled sentences\"\"\"\n",
    "        labeled_sentences = self.predict_soap_labels(transcript)\n",
    "        \n",
    "        soap_note = {\n",
    "            \"Subjective\": {\"Chief_Complaint\": \"\", \"History_of_Present_Illness\": \"\"},\n",
    "            \"Objective\": {\"Physical_Exam\": \"\", \"Observations\": \"\"},\n",
    "            \"Assessment\": {\"Diagnosis\": \"\", \"Severity\": \"\"},\n",
    "            \"Plan\": {\"Treatment\": \"\", \"Follow_Up\": \"\"}\n",
    "        }\n",
    "        \n",
    "        # Group sentences by label\n",
    "        for sentence, label in labeled_sentences:\n",
    "            if label == 'S-COMPLAINT':\n",
    "                soap_note[\"Subjective\"][\"Chief_Complaint\"] += sentence + \" \"\n",
    "            elif label == 'S-HISTORY':\n",
    "                soap_note[\"Subjective\"][\"History_of_Present_Illness\"] += sentence + \" \"\n",
    "            elif label == 'O-EXAM':\n",
    "                soap_note[\"Objective\"][\"Physical_Exam\"] += sentence + \" \"\n",
    "            elif label == 'O-OBS':\n",
    "                soap_note[\"Objective\"][\"Observations\"] += sentence + \" \"\n",
    "            elif label == 'A-DIAG':\n",
    "                soap_note[\"Assessment\"][\"Diagnosis\"] += sentence + \" \"\n",
    "            elif label == 'A-SEV':\n",
    "                soap_note[\"Assessment\"][\"Severity\"] += sentence + \" \"\n",
    "            elif label == 'P-TREAT':\n",
    "                soap_note[\"Plan\"][\"Treatment\"] += sentence + \" \"\n",
    "            elif label == 'P-FOLLOW':\n",
    "                soap_note[\"Plan\"][\"Follow_Up\"] += sentence + \" \"\n",
    "        \n",
    "        # Clean up extra spaces\n",
    "        for section in soap_note:\n",
    "            for subsection in soap_note[section]:\n",
    "                soap_note[section][subsection] = soap_note[section][subsection].strip()\n",
    "        \n",
    "        return soap_note\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# APPROACH 4: Hybrid System (Rule-Based + Transformer)\n",
    "# ============================================================================\n",
    "\n",
    "class HybridSOAPGenerator:\n",
    "    \"\"\"\n",
    "    Combines rule-based extraction with transformer refinement\n",
    "    Best for: Balanced accuracy and interpretability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rule_based = RuleBasedSOAPGenerator()\n",
    "        self.transformer = None  # Initialize when needed\n",
    "    \n",
    "    def generate_soap_note(self, transcript: str, use_transformer: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate SOAP note using hybrid approach\n",
    "        1. Extract with rules\n",
    "        2. Refine with transformer if available\n",
    "        \"\"\"\n",
    "        # First pass: Rule-based extraction\n",
    "        soap_note = self.rule_based.generate_soap_note(transcript)\n",
    "        \n",
    "        # Second pass: Transformer refinement (if enabled)\n",
    "        if use_transformer and self.transformer:\n",
    "            for section in soap_note:\n",
    "                section_text = json.dumps(soap_note[section])\n",
    "                refined = self.transformer.generate_section(transcript, section)\n",
    "                try:\n",
    "                    soap_note[section] = json.loads(refined)\n",
    "                except:\n",
    "                    pass  # Keep rule-based result if parsing fails\n",
    "        \n",
    "        return soap_note\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN USAGE AND DEMONSTRATION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    # Sample transcript\n",
    "    transcript = \"\"\"\n",
    "    Doctor: How are you feeling today?\n",
    "    Patient: I had a car accident. My neck and back hurt a lot for four weeks.\n",
    "    Doctor: Did you receive treatment?\n",
    "    Patient: Yes, I had ten physiotherapy sessions, and now I only have occasional back pain.\n",
    "    Doctor: Let me examine you. Your neck and back have full range of motion with no tenderness.\n",
    "    Doctor: Based on the examination, you have a whiplash injury and lower back strain. It's mild and improving.\n",
    "    Doctor: Continue physiotherapy as needed and use analgesics for pain relief.\n",
    "    Doctor: Return if pain worsens or persists beyond six months.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SOAP NOTE GENERATION DEMONSTRATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Approach 1: Rule-Based\n",
    "    print(\"\\n🔹 APPROACH 1: Rule-Based System\")\n",
    "    print(\"-\" * 80)\n",
    "    rule_generator = RuleBasedSOAPGenerator()\n",
    "    soap_note_1 = rule_generator.generate_soap_note(transcript)\n",
    "    print(json.dumps(soap_note_1, indent=2))\n",
    "    \n",
    "    # Approach 2: Transformer (demonstration)\n",
    "    print(\"\\n🔹 APPROACH 2: Transformer-Based (T5)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Note: Requires fine-tuning on medical SOAP note dataset\")\n",
    "    print(\"Example usage:\")\n",
    "    print(\"\"\"\n",
    "    transformer_gen = TransformerSOAPGenerator()\n",
    "    train_data, val_data = transformer_gen.prepare_training_data('soap_dataset.csv')\n",
    "    transformer_gen.train(train_data, val_data)\n",
    "    soap_note_2 = transformer_gen.generate_soap_note(transcript)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Approach 3: BERT Sequence Labeling (demonstration)\n",
    "    print(\"\\n🔹 APPROACH 3: BERT Token Classification\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Note: Requires annotated data with sentence-level SOAP labels\")\n",
    "    print(\"Example usage:\")\n",
    "    print(\"\"\"\n",
    "    bert_labeler = BERTSOAPLabeler()\n",
    "    train_data, val_data = bert_labeler.prepare_training_data(annotated_data)\n",
    "    bert_labeler.train(train_data, val_data)\n",
    "    soap_note_3 = bert_labeler.generate_soap_note(transcript)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Approach 4: Hybrid\n",
    "    print(\"\\n🔹 APPROACH 4: Hybrid System\")\n",
    "    print(\"-\" * 80)\n",
    "    hybrid_gen = HybridSOAPGenerator()\n",
    "    soap_note_4 = hybrid_gen.generate_soap_note(transcript, use_transformer=False)\n",
    "    print(json.dumps(soap_note_4, indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mednlp)",
   "language": "python",
   "name": "mednlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
